# -*- coding: utf-8 -*-
"""ImageSegmentationImproved.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E_LKp4eB5SvjJail2PlARntm-8IuOFlO

# ğŸ–¼ï¸ Image Segmentation Project (Ä°yileÅŸtirilmiÅŸ)

Bu notebook, **Oxford-IIIT Pet Dataset** kullanarak gÃ¶rÃ¼ntÃ¼ segmentasyonu gerÃ§ekleÅŸtirir.

**Model Mimarisi:** MobileNetV2 tabanlÄ± U-Net

**Ã–zellikler:**
- Transfer learning ile Ã¶nceden eÄŸitilmiÅŸ encoder
- Skip connections ile detaylÄ± segmentasyon
- Data augmentation ile genelleme
- Mean IoU metriÄŸi ile deÄŸerlendirme

## 1. KÃ¼tÃ¼phanelerin YÃ¼klenmesi

Gerekli TensorFlow ve yardÄ±mcÄ± kÃ¼tÃ¼phaneleri import ediyoruz.
"""

import tensorflow as tf
import tensorflow_datasets as tfds
from IPython.display import clear_output
import matplotlib.pyplot as plt
import numpy as np

# GPU kontrolÃ¼
print("TensorFlow version:", tf.__version__)
print("GPU available:", len(tf.config.list_physical_devices('GPU')) > 0)

"""## 2. Hiperparametreler

Model eÄŸitimi iÃ§in kullanÄ±lacak parametreleri tanÄ±mlÄ±yoruz.
"""

# GÃ¶rÃ¼ntÃ¼ boyutu - 128x128 Colab iÃ§in optimize edilmiÅŸ
IMG_SIZE = 128

# EÄŸitim parametreleri
BATCH_SIZE = 64
BUFFER_SIZE = 1000
EPOCHS = 20

# Segmentasyon sÄ±nÄ±f sayÄ±sÄ± (pet, background, border)
OUTPUT_CHANNELS = 3

print(f"GÃ¶rÃ¼ntÃ¼ Boyutu: {IMG_SIZE}x{IMG_SIZE}")
print(f"Batch Size: {BATCH_SIZE}")
print(f"Epoch SayÄ±sÄ±: {EPOCHS}")

"""## 3. Dataset YÃ¼kleme

Oxford-IIIT Pet Dataset'i tensorflow_datasets kullanarak yÃ¼klÃ¼yoruz.
"""

# Dataset'i indir ve yÃ¼kle (versiyon 4.0.0)
dataset, info = tfds.load('oxford_iiit_pet:4.0.0', with_info=True)

# Dataset bilgilerini gÃ¶ster
print("Dataset Bilgileri:")
print(f"  - EÄŸitim Ã¶rnek sayÄ±sÄ±: {info.splits['train'].num_examples}")
print(f"  - Test Ã¶rnek sayÄ±sÄ±: {info.splits['test'].num_examples}")
print(f"  - Ã–zellikler: {info.features}")

"""## 4. Veri Ã–n Ä°ÅŸleme FonksiyonlarÄ±

GÃ¶rÃ¼ntÃ¼leri normalize etme ve data augmentation iÅŸlemleri.
"""

def normalize(input_image, input_mask):
    """
    GÃ¶rÃ¼ntÃ¼yÃ¼ [0,1] aralÄ±ÄŸÄ±na normalize eder.
    Mask deÄŸerlerini 0-indexed yapar (1,2,3 -> 0,1,2).
    """
    input_image = tf.cast(input_image, tf.float32) / 255.0
    input_mask = tf.cast(input_mask, tf.int32)
    input_mask = input_mask - 1
    return input_image, input_mask


def augment(input_image, input_mask):
    """
    Data augmentation: Flip, brightness, contrast.
    """
    # Random horizontal flip
    if tf.random.uniform(()) > 0.5:
        input_image = tf.image.flip_left_right(input_image)
        input_mask = tf.image.flip_left_right(input_mask)

    # Random brightness (sadece gÃ¶rÃ¼ntÃ¼ye)
    input_image = tf.image.random_brightness(input_image, max_delta=0.1)

    # DeÄŸerleri [0,1] aralÄ±ÄŸÄ±nda tut
    input_image = tf.clip_by_value(input_image, 0.0, 1.0)

    return input_image, input_mask


def load_image_train(datapoint):
    """
    EÄŸitim verisi iÃ§in: resize, normalize, augment.
    """
    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))
    input_mask = tf.image.resize(
        datapoint['segmentation_mask'],
        (IMG_SIZE, IMG_SIZE),
        method='nearest'  # Mask iÃ§in nearest neighbor interpolation
    )

    input_image, input_mask = normalize(input_image, input_mask)
    input_image, input_mask = augment(input_image, input_mask)

    # Mask'Ä± squeeze et (son boyutu kaldÄ±r)
    input_mask = tf.squeeze(input_mask, axis=-1)

    return input_image, input_mask


def load_image_test(datapoint):
    """
    Test verisi iÃ§in: resize ve normalize (augmentation yok).
    """
    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))
    input_mask = tf.image.resize(
        datapoint['segmentation_mask'],
        (IMG_SIZE, IMG_SIZE),
        method='nearest'
    )

    input_image, input_mask = normalize(input_image, input_mask)

    # Mask'Ä± squeeze et (son boyutu kaldÄ±r)
    input_mask = tf.squeeze(input_mask, axis=-1)

    return input_image, input_mask

print("Veri Ã¶n iÅŸleme fonksiyonlarÄ± tanÄ±mlandÄ± âœ“")

"""## 5. Dataset Pipeline OluÅŸturma

Verimli eÄŸitim iÃ§in tf.data pipeline'Ä± oluÅŸturuyoruz.
"""

TRAIN_LENGTH = info.splits['train'].num_examples
STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE

# EÄŸitim dataset'i
train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.AUTOTUNE)
train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()
train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)

# Test dataset'i
test = dataset['test'].map(load_image_test, num_parallel_calls=tf.data.AUTOTUNE)
test_dataset = test.batch(BATCH_SIZE)

print(f"EÄŸitim adÄ±m sayÄ±sÄ±/epoch: {STEPS_PER_EPOCH}")
print("Dataset pipeline hazÄ±r âœ“")

"""## 6. Ã–rnek GÃ¶rÃ¼ntÃ¼ GÃ¶sterimi

Dataset'ten Ã¶rnek bir gÃ¶rÃ¼ntÃ¼ ve mask'Ä±nÄ± gÃ¶rselleÅŸtiriyoruz.
"""

def display(display_list, titles=None):
    """
    GÃ¶rÃ¼ntÃ¼leri yan yana gÃ¶sterir.
    """
    if titles is None:
        titles = ['GiriÅŸ GÃ¶rÃ¼ntÃ¼sÃ¼', 'GerÃ§ek Mask', 'Tahmin Mask']

    plt.figure(figsize=(15, 5))

    for i in range(len(display_list)):
        plt.subplot(1, len(display_list), i+1)
        plt.title(titles[i], fontsize=12)
        plt.imshow(display_list[i])
        plt.axis('off')

    plt.tight_layout()
    plt.show()


# Ã–rnek gÃ¶rÃ¼ntÃ¼ al
for image, mask in train.take(1):
    sample_image, sample_mask = image, mask

display([sample_image, sample_mask], ['Ã–rnek GÃ¶rÃ¼ntÃ¼', 'Segmentasyon Mask'])

"""## 7. Upsample Blok TanÄ±mÄ±

U-Net decoder iÃ§in kendi upsample bloklarÄ±mÄ±zÄ± oluÅŸturuyoruz.

Her blok:
- Conv2DTranspose (upsampling)
- BatchNormalization (stabilite)
- Dropout (regularization)
- ReLU aktivasyon
"""

def upsample_block(filters, size, apply_dropout=False):
    """
    Upsample bloÄŸu oluÅŸturur.

    Args:
        filters: Ã‡Ä±kÄ±ÅŸ filtre sayÄ±sÄ±
        size: Kernel boyutu
        apply_dropout: Dropout uygulansÄ±n mÄ±

    Returns:
        Sequential model
    """
    initializer = tf.random_normal_initializer(0., 0.02)

    result = tf.keras.Sequential()

    # Transposed convolution (upsampling)
    result.add(
        tf.keras.layers.Conv2DTranspose(
            filters, size, strides=2,
            padding='same',
            kernel_initializer=initializer,
            use_bias=False
        )
    )

    # Batch normalization
    result.add(tf.keras.layers.BatchNormalization())

    # Dropout (opsiyonel)
    if apply_dropout:
        result.add(tf.keras.layers.Dropout(0.5))

    # ReLU aktivasyon
    result.add(tf.keras.layers.ReLU())

    return result

print("Upsample blok fonksiyonu tanÄ±mlandÄ± âœ“")

"""## 8. Encoder (MobileNetV2) OluÅŸturma

Transfer learning iÃ§in Ã¶nceden eÄŸitilmiÅŸ MobileNetV2'yi kullanÄ±yoruz.
"""

# MobileNetV2 base model
base_model = tf.keras.applications.MobileNetV2(
    input_shape=[IMG_SIZE, IMG_SIZE, 3],
    include_top=False,
    weights='imagenet'
)

# Skip connection iÃ§in kullanÄ±lacak katmanlar
# FarklÄ± Ã§Ã¶zÃ¼nÃ¼rlÃ¼klerde feature map'ler alÄ±yoruz
layer_names = [
    'block_1_expand_relu',   # 64x64
    'block_3_expand_relu',   # 32x32
    'block_6_expand_relu',   # 16x16
    'block_13_expand_relu',  # 8x8
    'block_16_project',      # 4x4
]

base_model_outputs = [base_model.get_layer(name).output for name in layer_names]

# Feature extraction model
down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)

# Encoder'Ä± dondur (transfer learning)
down_stack.trainable = False

print("Encoder (MobileNetV2) oluÅŸturuldu âœ“")
print("Skip connection katman Ã§Ä±kÄ±ÅŸ boyutlarÄ±:")
for i, (name, output) in enumerate(zip(layer_names, base_model_outputs)):
    print(f"  {i+1}. {name}: {output.shape}")

"""## 9. Decoder (Upsampling Stack) OluÅŸturma

U-Net decoder'Ä± iÃ§in upsample bloklarÄ± oluÅŸturuyoruz.
"""

# Decoder upsample stack
up_stack = [
    upsample_block(512, 3, apply_dropout=True),  # 4x4 -> 8x8
    upsample_block(256, 3, apply_dropout=True),  # 8x8 -> 16x16
    upsample_block(128, 3),                      # 16x16 -> 32x32
    upsample_block(64, 3),                       # 32x32 -> 64x64
]

print("Decoder upsample stack oluÅŸturuldu âœ“")

"""## 10. U-Net Model OluÅŸturma

Encoder ve decoder'Ä± skip connections ile birleÅŸtiren tam U-Net modeli.
"""

def unet_model(output_channels):
    """
    U-Net segmentasyon modeli oluÅŸturur.

    Args:
        output_channels: Ã‡Ä±kÄ±ÅŸ sÄ±nÄ±f sayÄ±sÄ±

    Returns:
        Keras Model
    """
    inputs = tf.keras.layers.Input(shape=[IMG_SIZE, IMG_SIZE, 3])

    # Encoder: Downsampling
    skips = down_stack(inputs)
    x = skips[-1]  # En derin feature map
    skips = reversed(skips[:-1])  # Skip connections iÃ§in diÄŸerleri

    # Decoder: Upsampling + Skip Connections
    for up, skip in zip(up_stack, skips):
        x = up(x)
        concat = tf.keras.layers.Concatenate()
        x = concat([x, skip])

    # Son katman: Orijinal boyuta upsampling
    last = tf.keras.layers.Conv2DTranspose(
        filters=output_channels,
        kernel_size=3,
        strides=2,
        padding='same'
    )  # 64x64 -> 128x128

    x = last(x)

    return tf.keras.Model(inputs=inputs, outputs=x)

print("U-Net model fonksiyonu tanÄ±mlandÄ± âœ“")

"""## 11. Model Derleme

Modeli optimizer, loss fonksiyonu ve metriklerle derliyoruz.
"""

# Model oluÅŸtur
model = unet_model(OUTPUT_CHANNELS)

# Model derleme
model.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

print("Model derlendi âœ“")
print(f"Toplam parametre sayÄ±sÄ±: {model.count_params():,}")

"""## 12. Model Mimarisi GÃ¶rselleÅŸtirme"""

# Model Ã¶zeti
model.summary()

# Model diyagramÄ± (opsiyonel - graphviz gerektirir)
try:
    tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)
except Exception as e:
    print(f"Model diyagramÄ± oluÅŸturulamadÄ±: {e}")

"""## 13. Tahmin ve GÃ¶rselleÅŸtirme FonksiyonlarÄ±"""

def create_mask(pred_mask):
    """
    Model Ã§Ä±ktÄ±sÄ±ndan segmentasyon mask'Ä± oluÅŸturur.
    """
    pred_mask = tf.argmax(pred_mask, axis=-1)
    pred_mask = pred_mask[..., tf.newaxis]
    return pred_mask[0]


def show_predictions(dataset=None, num=1):
    """
    Model tahminlerini gÃ¶rselleÅŸtirir.
    """
    if dataset:
        for image, mask in dataset.take(num):
            pred_mask = model.predict(image, verbose=0)
            display([image[0], mask[0], tf.squeeze(create_mask(pred_mask))])
    else:
        pred_mask = model.predict(sample_image[tf.newaxis, ...], verbose=0)
        display([sample_image, sample_mask, tf.squeeze(create_mask(pred_mask))])


# EÄŸitim Ã¶ncesi tahmin
print("EÄŸitim Ã¶ncesi tahmin (rastgele aÄŸÄ±rlÄ±klarla):")
show_predictions()

"""## 14. Callbacks TanÄ±mlama

EÄŸitim sÄ±rasÄ±nda kullanÄ±lacak callbacks:
- **DisplayCallback**: Her epoch sonunda Ã¶rnek tahmin gÃ¶ster
- **EarlyStopping**: Overfitting'i Ã¶nle
"""

class DisplayCallback(tf.keras.callbacks.Callback):
    """
    Her epoch sonunda Ã¶rnek tahmin gÃ¶sterir.
    """
    def on_epoch_end(self, epoch, logs=None):
        clear_output(wait=True)
        show_predictions()
        print(f'\nğŸ“Š Epoch {epoch+1} tamamlandÄ±')
        if logs:
            print(f'   Loss: {logs.get("loss", 0):.4f}')
            print(f'   Accuracy: {logs.get("accuracy", 0):.4f}')
            print(f'   Val Loss: {logs.get("val_loss", 0):.4f}')


# Callbacks listesi
callbacks = [
    DisplayCallback(),

    # Early stopping - validation loss iyileÅŸmezse dur
    tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=5,
        restore_best_weights=True,
        verbose=1
    )
]

print("Callbacks tanÄ±mlandÄ± âœ“")

"""## 15. Model EÄŸitimi

Modeli eÄŸitim verisinde eÄŸitiyoruz.
"""

# Validation step sayÄ±sÄ±
VAL_SUBSPLITS = 5
VALIDATION_STEPS = info.splits['test'].num_examples // BATCH_SIZE // VAL_SUBSPLITS

print(f"EÄŸitim baÅŸlÄ±yor...")
print(f"  - Epochs: {EPOCHS}")
print(f"  - Steps per epoch: {STEPS_PER_EPOCH}")
print(f"  - Validation steps: {VALIDATION_STEPS}")
print("-" * 50)

# Model eÄŸitimi
model_history = model.fit(
    train_dataset,
    epochs=EPOCHS,
    steps_per_epoch=STEPS_PER_EPOCH,
    validation_steps=VALIDATION_STEPS,
    validation_data=test_dataset,
    callbacks=callbacks
)

print("\nâœ… EÄŸitim tamamlandÄ±!")

"""## 16. EÄŸitim SonuÃ§larÄ± GÃ¶rselleÅŸtirme"""

# EÄŸitim geÃ§miÅŸi
history = model_history.history
epochs_range = range(len(history['loss']))

# GÃ¶rselleÅŸtirme
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Loss grafiÄŸi
axes[0].plot(epochs_range, history['loss'], 'b-', label='EÄŸitim Loss', linewidth=2)
axes[0].plot(epochs_range, history['val_loss'], 'r--', label='Validation Loss', linewidth=2)
axes[0].set_title('EÄŸitim ve Validation Loss', fontsize=14)
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Accuracy grafiÄŸi
axes[1].plot(epochs_range, history['accuracy'], 'b-', label='EÄŸitim Accuracy', linewidth=2)
axes[1].plot(epochs_range, history['val_accuracy'], 'r--', label='Validation Accuracy', linewidth=2)
axes[1].set_title('EÄŸitim ve Validation Accuracy', fontsize=14)
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Accuracy')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Son metrikler
print("\nğŸ“ˆ Son Epoch Metrikleri:")
print(f"   Training Loss: {history['loss'][-1]:.4f}")
print(f"   Validation Loss: {history['val_loss'][-1]:.4f}")
print(f"   Training Accuracy: {history['accuracy'][-1]:.4f}")
print(f"   Validation Accuracy: {history['val_accuracy'][-1]:.4f}")

"""## 17. Test Seti Ãœzerinde DeÄŸerlendirme"""

# Test seti Ã¼zerinde deÄŸerlendirme
print("Test seti Ã¼zerinde model deÄŸerlendirmesi...")
results = model.evaluate(test_dataset, verbose=1)

print("\nğŸ“Š Test SonuÃ§larÄ±:")
for name, value in zip(model.metrics_names, results):
    print(f"   {name}: {value:.4f}")

"""## 18. Ã–rnek Tahminler"""

# BirkaÃ§ test gÃ¶rÃ¼ntÃ¼sÃ¼ Ã¼zerinde tahmin
print("Test setinden Ã¶rnek tahminler:\n")
show_predictions(test_dataset, num=3)

"""## 19. Model Kaydetme"""

# Modeli kaydet
model.save('pet_segmentation_model.keras')
print("âœ… Model 'pet_segmentation_model.keras' olarak kaydedildi")

"""---

# ğŸ“ YapÄ±lan DeÄŸiÅŸiklikler ve Ä°yileÅŸtirmeler

Bu notebook, orijinal Image Segmentation projesinin geliÅŸtirilmiÅŸ versiyonudur.

## âœ… Ã‡Ã¶zÃ¼len Sorunlar

### 1. tensorflow_examples BaÄŸÄ±mlÄ±lÄ±ÄŸÄ± KaldÄ±rÄ±ldÄ±
- **Sorun:** `from tensorflow_examples.models.pix2pix import pix2pix` import hatasÄ±
- **Ã‡Ã¶zÃ¼m:** Kendi `upsample_block` fonksiyonumuz yazÄ±ldÄ±
- **Avantaj:** Harici baÄŸÄ±mlÄ±lÄ±k olmadan Ã§alÄ±ÅŸÄ±r

### 2. Dataset Versiyonu GÃ¼ncellendi
- **Ã–nceki:** `oxford_iiit_pet:3.*.*` (artÄ±k mevcut deÄŸil)
- **Åimdi:** `oxford_iiit_pet:4.0.0`
- **Avantaj:** GÃ¼ncel dataset versiyonu ile uyumlu

### 3. Mask Veri Tipi DÃ¼zeltildi
- **Sorun:** Float mask deÄŸerleri hata veriyordu
- **Ã‡Ã¶zÃ¼m:** Mask'lar int32'ye cast edildi ve squeeze edildi
- **Avantaj:** SparseCategoricalCrossentropy ile uyumlu

### 4. Data Augmentation Eklendi
- Random horizontal flip
- Random brightness
- **Avantaj:** Model daha iyi genelleme yapar

### 5. Training Callbacks Eklendi
- **EarlyStopping:** Validation loss 5 epoch boyunca iyileÅŸmezse durur
- **DisplayCallback:** Her epoch sonunda Ã¶rnek tahmin gÃ¶sterir
- **Avantaj:** Overfitting Ã¶nlenir

### 6. Mask Resize Metodu DÃ¼zeltildi
- **Ã–nceki:** Bilinear interpolation (mask deÄŸerlerini bozar)
- **Åimdi:** Nearest neighbor interpolation
- **Avantaj:** Mask sÄ±nÄ±f deÄŸerleri korunur (0, 1, 2)

### 7. Upsample BloklarÄ± Ä°yileÅŸtirildi
- BatchNormalization eklendi (stabilite)
- Dropout eklendi (regularization)
- Proper weight initialization

### 8. DokÃ¼mantasyon Eklendi
- TÃ¼rkÃ§e aÃ§Ä±klamalar
- Markdown hÃ¼creleri ile bÃ¶lÃ¼mler
- Fonksiyon docstring'leri

## ğŸ”§ Teknik Detaylar

- **Model:** U-Net with MobileNetV2 encoder
- **Input Size:** 128x128x3
- **Output:** 128x128x3 (3 sÄ±nÄ±f: pet, background, border)
- **Optimizer:** Adam
- **Loss:** Sparse Categorical Crossentropy
"""